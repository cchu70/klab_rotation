{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import pickle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "if not os.path.exists('./../convolutional/cifar-10-batches-py/train'):\n",
    "    train = np.empty([0,3073])\n",
    "    for i in ([1,2,3,4,5]):\n",
    "        t = unpickle('./../convolutional/cifar-10-batches-py/data_batch_{}'.format(i))\n",
    "        train = np.vstack((train,np.hstack((t.get(b'data'),np.array(t.get(b'labels')).reshape(10000,1)))))\n",
    "    t = unpickle('./../convolutional/cifar-10-batches-py/test_batch')\n",
    "    test = np.hstack((t.get(b'data'),np.array(t.get(b'labels')).reshape(10000,1)))\n",
    "    with open('./../convolutional/cifar-10-batches-py/train', 'wb') as file:\n",
    "        pickle.dump(train, file)\n",
    "    with open('./../convolutional/cifar-10-batches-py/test', 'wb') as file:\n",
    "        pickle.dump(test, file)\n",
    "    \n",
    "else:\n",
    "    with open('./../convolutional/cifar-10-batches-py/train','rb') as file:\n",
    "        train = pickle.load(file)\n",
    "    with open('./../convolutional/cifar-10-batches-py/test','rb') as file:\n",
    "        test = pickle.load(file)\n",
    "#test =  pd.read_csv('./MNIST_CSV/mnist_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[158, 159, 165, ..., 129, 110,   3],\n",
       "       [235, 231, 232, ..., 191, 199,   8],\n",
       "       [158, 158, 139, ...,   3,   7,   8],\n",
       "       ...,\n",
       "       [ 20,  19,  15, ...,  53,  47,   5],\n",
       "       [ 25,  15,  23, ...,  81,  80,   1],\n",
       "       [ 73,  98,  99, ...,  58,  26,   7]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y : torch.eye(10)[y])\n",
    "transform = Lambda(lambda y : torch.reshape(y,(3,32,32)))\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = torch.from_numpy(annotations_file)\n",
    "        self.img = torch.from_numpy(img_dir)\n",
    "        self.target_transform = target_transform\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img[idx,:]\n",
    "        label = self.img_labels[idx]\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        if self.transform:\n",
    "            image = self.transform(label)\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "train_dataloader = DataLoader(CustomImageDataset(train[:,-1],train[:,:-1],target_transform = target_transform), batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(CustomImageDataset(test[:,-1],test[:,:-1],target_transform = target_transform), batch_size=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,3,4,1).float()\n",
    "        self.conv2 = nn.Conv2d(3,9,4,1).float()\n",
    "        self.conv3 = nn.Conv2d(9,18, 7, 1).float()\n",
    "\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(4608, 128).float()\n",
    "        self.fc2 = nn.Linear(128, 10).float()\n",
    "        #self.fc3 = nn.Linear(10,1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "      \n",
    "    def forward(self, input):\n",
    "\n",
    "        x = F.relu(self.conv1(input))\n",
    "        x = self.dropout1(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(self.dropout2(F.relu(self.conv3(x))), start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x.float()\n",
    "\n",
    "    def back(self,D):\n",
    "        i = 0\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.named_parameters():\n",
    "                param.where(D[i].int().bool(), torch.tensor(0.0))\n",
    "                i += 1\n",
    "            \"\"\"\n",
    "            D_weight = torch.from_numpy(D[0][:-1,:].T.astype(int)).bool()\n",
    "            D_bias = torch.from_numpy(D[0][-1,:].T.astype(int)).bool()\n",
    "            self.fc1.bias.where(D_bias, torch.tensor(0.0))\n",
    "            self.fc1.weight.where(D_weight, torch.tensor(0.0))\n",
    "            D_weight = torch.from_numpy(D[1][:-1,:].T.astype(int)).bool()\n",
    "            D_bias = torch.from_numpy(D[1][-1,:].T.astype(int)).bool()\n",
    "            self.fc2.bias.where(D_bias, torch.tensor(0.0))\n",
    "            self.fc2.weight.where(D_weight, torch.tensor(0.0))\n",
    "            D_weight = torch.from_numpy(D[2][:-1,:].T.astype(int)).bool()\n",
    "            D_bias = torch.from_numpy(D[2][-1,:].T.astype(int)).bool()\n",
    "            self.fc3.bias.where(D_bias, torch.tensor(0.0))\n",
    "            self.fc3.weight.where(D_weight, torch.tensor(0.0))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4865/558160761.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc2(x))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m output \u001b[39m=\u001b[39m model(X\u001b[39m.\u001b[39mreshape(bat,\u001b[39m1\u001b[39m,\u001b[39m28\u001b[39m,\u001b[39m28\u001b[39m)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat())\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(Y\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat(),output\u001b[39m.\u001b[39msqueeze())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m#wandb.log({\"loss\":loss})\u001b[39;00m\n",
      "File \u001b[0;32m~/Anaconda3/envs/klab_proj/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/Anaconda3/envs/klab_proj/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"for mom in (0.004,0.001, 0.0009,0.0005):\n",
    "    for bat in (100,500, 1000):\n",
    "        for lr in (0.01,0.004,0.001, 0.0009,0.0005,0.0001):\n",
    "            for ep in (10,15,30,50):\n",
    "                wandb.init(\n",
    "                    project=\"my-awesome-project\",\n",
    "                    config={\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"architecture\": \"fully_connected\",\n",
    "                            \"dataset\": \"MNIST\",\n",
    "                            \"epochs\": ep,\n",
    "                            \"momentum\":mom,\n",
    "                            \"batch_size\":bat,\n",
    "                            })\"\"\"\n",
    "bat = 100\n",
    "ep = 15\n",
    "lr = 0.004\n",
    "mom = 0.0009\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_dataloader = DataLoader(CustomImageDataset(train[:,-1],train[:,:-1],target_transform = target_transform), batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(CustomImageDataset(test[:,-1],test[:,:-1],target_transform = target_transform), batch_size=100, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=mom)\n",
    "for ep in range(ep):\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            X, Y = data\n",
    "            optim.zero_grad()\n",
    "            output = model(X.reshape(bat,1,28,28).to(device).float()).float()\n",
    "            \n",
    "            loss = criterion(Y.to(device).float(),output.squeeze())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            #wandb.log({\"loss\":loss})\n",
    "            \"\"\"if i%500 ==0:\n",
    "                print(loss.float(),output[0].argmax(),Y[0].argmax(),ep)\"\"\"\n",
    "out = []\n",
    "y = []\n",
    "for i, data in enumerate(test_dataloader, 0):\n",
    "    X, Y = data    \n",
    "    output = model.forward(X.reshape(bat,1,28,28).to(device).float()).float()\n",
    "    out.extend(output.cpu().detach().float().numpy())\n",
    "    y.extend(Y)\n",
    "\n",
    "\n",
    "print(list(np.subtract([i.argmax()for i in out],[i.argmax()for i in y])).count(0)/len(y),mom, lr, bat, ep)\n",
    "#                wandb.log({\"accuracy\":list(np.subtract([i.argmax()for i in out],[i.argmax()for i in y])).count(0)/len(y)})\n",
    "                \n",
    "\"\"\"\n",
    "PATH = './MNIST.pth'\n",
    "torch.save(model.state_dict(), PATH)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(D, train_dataloader, test_dataloader ,model, epoch,i):\n",
    "    epochs = epoch\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=i, momentum=0.0009)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.back(D) \n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            X, Y = data\n",
    "            optim.zero_grad()\n",
    "            output = model(X.reshape(100,1,28,28).to(device).float()).float()\n",
    "            loss = criterion(Y.to(device).float(),output.squeeze())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            model.back(D)\n",
    "            \n",
    "    w = []\n",
    "    for name, param in model.named_parameters():\n",
    "        w.append(param.cpu().detach().numpy())\n",
    "        \n",
    "    out = []\n",
    "    y = []\n",
    "    for i, data in enumerate(test_dataloader, 0):\n",
    "        X, Y = data    \n",
    "        output = model.forward(X.reshape(100,1,28,28).to(device).float()).float()\n",
    "        out.extend(output.cpu().detach().float().numpy())\n",
    "        y.extend(Y.cpu())\n",
    "    non_z = 0\n",
    "    tot = 0\n",
    "    for i in D:\n",
    "        non_z += int(torch.count_nonzero(i))\n",
    "        tot += torch.numel(i)\n",
    "    print(tot)\n",
    "    return(w,list(np.subtract([i.argmax()for i in out],[i.argmax()for i in y])).count(0)/len(y), non_z/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(N,model):\n",
    "    W = []\n",
    "    D = []\n",
    "    for name, param in model.named_parameters():\n",
    "        weight = param.cpu().detach()\n",
    "        list_null = torch.tensor([random.randrange(1,(torch.numel(param)), 1) for j in range(2*int((torch.numel(param))/3))])\n",
    "        print(list_null.shape)\n",
    "        dead = torch.zeros(torch.numel(param))\n",
    "        dead[list_null] = 1\n",
    "        weight = weight.reshape(-1)\n",
    "        weight[list_null] = 0\n",
    "        weight = weight.reshape(param.shape).to(device)\n",
    "        dead = dead.reshape(param.shape).to(device)\n",
    "        W.append(weight)\n",
    "        D.append(dead)\n",
    "    \n",
    "    return W,D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomChange(count,D):\n",
    "    \n",
    "    size = D.shape\n",
    "    if torch.count_nonzero(D)<=count:\n",
    "        count = torch.count_nonzero(D)-1\n",
    "    wake_up = np.zeros(D.numel())\n",
    "    print(D.count_nonzero(),count)\n",
    "    wake_up[random.sample(list(np.where(D.reshape(-1) == 1)[0]), count)] = 1\n",
    "    dead = D.reshape(-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return torch.where(dead < torch.from_numpy(wake_up) ,dead,0).reshape(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prune(W,d,count):\n",
    "    indices = np.indices(W.shape).reshape(W.ndim,-1).T\n",
    "    w = np.absolute(W).reshape(-1)\n",
    "    w = np.hstack((w.reshape(w.shape[0],1),indices))\n",
    "    w = w[w[:,0].argsort()].astype(int)\n",
    "    \n",
    "    for i in range(count):\n",
    "        if i>=w.shape[0]:\n",
    "            break\n",
    "        while d[tuple(w[i,1:])] == 1:#to double check if correct\n",
    "            w = w[1:,:]\n",
    "            if i>=w.shape[0]:\n",
    "                break\n",
    "        if i>=w.shape[0]:\n",
    "            break\n",
    "        d[tuple(w[i,1:])] = 1\n",
    "        W[tuple(w[i,1:])] = 0\n",
    "    return W, d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SynapseSearch(gamma, nab, N):\n",
    "    epsilon = 1\n",
    "    d = 0\n",
    "    m =3\n",
    "    gam = gamma\n",
    "    #setweights\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    model = Net()\n",
    "    W,D = init(N,model)\n",
    "    \n",
    "    train_dataloader = DataLoader(CustomImageDataset(train[:,-1],train[:,:-1],target_transform = target_transform), batch_size=100, shuffle=True)\n",
    "    test_dataloader = DataLoader(CustomImageDataset(test[:,-1],test[:,:-1],target_transform = target_transform), batch_size=100, shuffle=True)\n",
    "\n",
    "    Acc = []\n",
    "    density = []\n",
    "    W, acc,dens = Train(D, train_dataloader, test_dataloader, model, 0, 0.004)\n",
    "    print(acc, 1-dens)\n",
    "    Acc.append(acc)\n",
    "    density.append(1-dens)\n",
    "    for i in range(1,int(nab+N/gamma)):\n",
    "        d = np.random.binomial(size=1, n=1, p= epsilon)\n",
    "        if d == 1:\n",
    "            for l in range(m):    \n",
    "                D[l] = randomChange(int(gam*(torch.numel(D[l]))),D[l])\n",
    "        else:\n",
    "            for l in range(m):\n",
    "                W[l], D[l] = Prune(W[l], D[l],int(gam*(torch.numel(D[l]))))\n",
    "        W , acc , dens = Train(D, train_dataloader, test_dataloader, model,2, 0.004)\n",
    "        #wandb.log({\"accuracy\":acc, \"density\":dens})\n",
    "\n",
    "        print(acc, 1-dens)\n",
    "        Acc.append(acc)\n",
    "        density.append(1-dens)\n",
    "        epsilon = max(0,epsilon-1/nab)\n",
    "        print(epsilon, i, )\n",
    "    plt.plot(Acc)\n",
    "    plt.show()\n",
    "    plt.plot(density)\n",
    "    return Acc, density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "torch.Size([2])\n",
      "torch.Size([288])\n",
      "torch.Size([6])\n",
      "torch.Size([5292])\n",
      "torch.Size([12])\n",
      "torch.Size([393216])\n",
      "torch.Size([84])\n",
      "torch.Size([852])\n",
      "torch.Size([6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97965/3580032988.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc2(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599690\n",
      "0.0975 0.5136637262585669\n",
      "tensor(26) 9\n",
      "tensor(1) 0\n",
      "tensor(212) 86\n",
      "599690\n",
      "0.7964 0.5140622655038436\n",
      "0.9 1\n",
      "tensor(0) tensor(-1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"wandb.init(\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m                    project=\"fully-connected\",\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m                    config={\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m                            \"N\":N,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m                            })\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m Acc, dens \u001b[39m=\u001b[39m SynapseSearch(gamma,nab, N)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#wandb.finish()\u001b[39;00m\n",
      "\u001b[1;32m/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(m):    \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         D[l] \u001b[39m=\u001b[39m randomChange(\u001b[39mint\u001b[39;49m(gam\u001b[39m*\u001b[39;49m(torch\u001b[39m.\u001b[39;49mnumel(D[l]))),D[l])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(m):\n",
      "\u001b[1;32m/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m wake_up \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(D\u001b[39m.\u001b[39mnumel())\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(D\u001b[39m.\u001b[39mcount_nonzero(),count)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m wake_up[random\u001b[39m.\u001b[39;49msample(\u001b[39mlist\u001b[39;49m(np\u001b[39m.\u001b[39;49mwhere(D\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m==\u001b[39;49m \u001b[39m1\u001b[39;49m)[\u001b[39m0\u001b[39;49m]), count)] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dead \u001b[39m=\u001b[39m D\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bastienll/Documents/EPFL/cours/Master/harvard/fully_connected/developemental_extended.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mwhere(dead \u001b[39m<\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(wake_up) ,dead,\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mreshape(size)\n",
      "File \u001b[0;32m~/Anaconda3/envs/klab_proj/lib/python3.12/random.py:430\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    428\u001b[0m randbelow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow\n\u001b[1;32m    429\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m k \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n:\n\u001b[0;32m--> 430\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSample larger than population or is negative\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m result \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m k\n\u001b[1;32m    432\u001b[0m setsize \u001b[39m=\u001b[39m \u001b[39m21\u001b[39m        \u001b[39m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "gamma = 0.2\n",
    "nab = 10\n",
    "N = 2\n",
    "\"\"\"wandb.init(\n",
    "                    project=\"fully-connected\",\n",
    "                    config={\n",
    "                            \"learning_rate\": 0.004,\n",
    "                            \"architecture\": \"fully_connected\",\n",
    "                            \"dataset\": \"MNIST\",\n",
    "                            \"momentum\":0.0009,\n",
    "                            \"batch_size\":100,\n",
    "                            \"gamma\":gamma,\n",
    "                            \"nab\":nab,\n",
    "                            \"N\":N,\n",
    "                            })\"\"\"\n",
    "Acc, dens = SynapseSearch(gamma,nab, N)\n",
    "#wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(torch.tensor([0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
