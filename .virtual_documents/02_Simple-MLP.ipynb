


%load_ext autoreload
%autoreload 2


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import metrics


import tensorflow as tf
from tensorflow import keras





# already normalized, scaled, and flattened
loaded = np.load('./data/01.MNIST.norm_scaled.train_test.npz')
split_X_train = loaded['split_X_train']
split_y_train = loaded['split_y_train']
split_X_test = loaded['split_X_test']
split_y_test = loaded['split_y_test']


split_X_train.shape


X_val, X_train = split_X_train[:1000], split_X_train[1000:]
y_val, y_train = split_y_train[:1000], split_y_train[1000:]


X_val.shape, X_train.shape


y_val.shape, y_train.shape





model = keras.models.Sequential([
    keras.layers.Input(shape=(784,)), # package prefers this to be the first layer. If still image/2D, can add a Flatten() layer
    keras.layers.Dense(16, activation='relu'), # not sure if this is correct?
    keras.layers.Dense(10, activation='softmax'), # output label
])


model.summary()


weights, biases = model.get_layer('dense_6').get_weights()
print(weights.shape)
print(weights)

print(biases.shape)
print(biases)





# compile
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='sgd',
    metrics=['accuracy'] # what other metrics?
)


# Train
history = model.fit(
    X_train, y_train,
    epochs=30,
    validation_data=(X_val, y_val)
)


history_df = pd.DataFrame(history.history)
history_df.head()


history_df.plot()
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.xlabel('Epochs')
plt.ylabel('Metrics')
plt.show()


# save model
model_weights_fn = './data/02.model.weights.h5'
model.save_weights(model_weights_fn)





model = keras.models.Sequential([
    keras.layers.Input(shape=(784,)), # package prefers this to be the first layer. If still image/2D, can add a Flatten() layer
    keras.layers.Dense(16, activation='relu'), # not sure if this is correct?
    keras.layers.Dense(10, activation='softmax'), # output label
])
model.load_weights('./data/02.model.weights.h5')


y_pred = model.predict(split_X_test)


argmax_y_pred = np.argmax(y_pred, axis=1) # get final classification


print(f"Accuracy: {metrics.accuracy_score(y_true=split_y_test, y_pred=argmax_y_pred)}")


display_idx = np.random.randint(len(split_y_test), size=5)
num_subplots = len(display_idx)
height = 5
fig, axes = plt.subplots(1, num_subplots, figsize=(num_subplots * height, height))
for i, di in enumerate(display_idx):
    axes[i].imshow(split_X_test[di].reshape(28,28), cmap='gray') # normalized
    axes[i].set_title(f"Predicted: {argmax_y_pred[di]}, Expected: {split_y_test[di]}")
plt.show()


wrong_pred_idx = np.argwhere(split_y_test!=argmax_y_pred).flatten()[:5]
num_subplots = len(wrong_pred_idx)
height = 5
fig, axes = plt.subplots(1, num_subplots, figsize=(num_subplots * height, height))
for i, di in enumerate(wrong_pred_idx):
    axes[i].imshow(split_X_test[di].reshape(28,28), cmap='gray') # normalized
    axes[i].set_title(f"Predicted: {argmax_y_pred[di]}, Expected: {split_y_test[di]}")
plt.show()


confusion_matrix = metrics.confusion_matrix(y_true=split_y_test, y_pred=argmax_y_pred, labels=np.arange(10))


disp = metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=np.arange(10))
disp.plot()
plt.show()



